{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Tz6ofj02_e_"
   },
   "source": [
    "This is Hand Written Text Recognition Project. We will be using **IAM data set**.\n",
    "\n",
    "Link to dataset: [https://fki.tic.heia-fr.ch/databases/iam-handwriting-database]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwNLQ2S53iY4"
   },
   "source": [
    "## **Data Collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Yf0Ntmh6Ff1"
   },
   "source": [
    "1.Download the IAM Words dataset ZIP file.\n",
    "\n",
    "2.Extract the ZIP file.\n",
    "\n",
    "3.Create necessary directories (data and data/Words).\n",
    "\n",
    "4.Extract the contents of the tar archive (words.tgz) into the data/Words directory.\n",
    "\n",
    "5.Move the words.txt file to the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3r5vrFGc3aUV",
    "outputId": "446c8437-f411-419f-b7f0-dd7139d2f6f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#drive_folder = '/content/drive/My Drive/Colab Notebooks/HandWritten document recognition'\n",
    "\n",
    "#!wget -q https://git.io/J0fjL -O \"{drive_folder}/IAM_Words.zip\"\n",
    "#!unzip -qq \"{drive_folder}/IAM_Words.zip\" -d \"{drive_folder}\"\n",
    "#!mkdir -p \"{drive_folder}/data/Words\"\n",
    "#!tar -xf \"{drive_folder}/IAM_Words/words.tgz\" -C \"{drive_folder}/data/Words\"\n",
    "#!mv \"{drive_folder}/IAM_Words/words.txt\" \"{drive_folder}/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OdZNIS4YI89m"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGU3_lS_oJrj",
    "outputId": "184c507c-1abb-4b08-b67f-11db7bb5392c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iRJeTCL-IykW"
   },
   "outputs": [],
   "source": [
    "words_file_path = \"./words.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvPsM1H2HP9g"
   },
   "source": [
    " **Getting Words from words.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IIK4oU_z3n0M",
    "outputId": "2e3a5fbd-0d13-4cb0-eed9-61deb820a1dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96456\n",
      "['a01-000u-00-00 ok 154 408 768 27 51 AT A\\n', 'a01-000u-00-01 ok 154 507 766 213 48 NN MOVE\\n', 'a01-000u-00-02 ok 154 796 764 70 50 TO to\\n', 'a01-000u-00-03 ok 154 919 757 166 78 VB stop\\n', 'a01-000u-00-04 ok 154 1185 754 126 61 NPT Mr.\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(words_file_path, 'r') as file:\n",
    "    words = file.readlines()\n",
    "\n",
    "words_list = []\n",
    "\n",
    "for line in words:\n",
    "  if line[0]==\"#\":\n",
    "    continue\n",
    "  if line.split(\" \")[1] != \"err\":  # we do not need to deal with errored entries.\n",
    "    words_list.append(line)\n",
    "\n",
    "print(len(words_list))\n",
    "print(words_list[:5])\n",
    "\n",
    "np.random.shuffle(words_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IRfzXBZ9G-l5",
    "outputId": "74637dea-4519-403f-d8e4-e59544ec704e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data samples: 96456\n",
      "Total training samples: 86810\n",
      "Total validation samples: 4823\n",
      "Total test samples: 4823\n"
     ]
    }
   ],
   "source": [
    "split_idx = int(0.9 * len(words_list))\n",
    "train_samples = words_list[:split_idx]\n",
    "test_samples = words_list[split_idx:]\n",
    "\n",
    "val_split_idx = int(0.5 * len(test_samples))\n",
    "validation_samples = test_samples[:val_split_idx]\n",
    "test_samples = test_samples[val_split_idx:]\n",
    "\n",
    "assert len(words_list) == len(train_samples) + len(validation_samples) + len(test_samples)\n",
    "\n",
    "print(f\"Total data samples: {len(words_list)}\")\n",
    "print(f\"Total training samples: {len(train_samples)}\")\n",
    "print(f\"Total validation samples: {len(validation_samples)}\")\n",
    "print(f\"Total test samples: {len(test_samples)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCfKXnHGLdSE"
   },
   "source": [
    "# **Data Input Pipeline**\n",
    "Our data images are not organized.\n",
    "\n",
    "We will build our data input pipeline by first preparing the image paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jx5NMKgjLn4s"
   },
   "outputs": [],
   "source": [
    "base_image_path = \"./words\"\n",
    "\n",
    "def get_image_paths_and_labels(samples):\n",
    "  paths = []\n",
    "  corrected_samples = []\n",
    "  for(i,file_line) in enumerate(samples):\n",
    "    line_split = file_line.strip()\n",
    "    line_split = line_split.split(\" \")\n",
    "    #each file_line is as \"a01-000u-00-03 ok 154 919 757 166 78 VB stop\"\n",
    "    # part1/part1-part2/part1-part2-part3.png\n",
    "\n",
    "    image_name = line_split[0]\n",
    "    partI = image_name.split(\"-\")[0]\n",
    "    partII = image_name.split(\"-\")[1]\n",
    "    img_path = os.path.join(\n",
    "        base_image_path, partI , partI + \"-\" + partII, image_name + \".png\"\n",
    "    )\n",
    "\n",
    "\n",
    "    if os.path.getsize(img_path):\n",
    "      paths.append(img_path)\n",
    "      corrected_samples.append(file_line.split(\"\\n\")[0])\n",
    "\n",
    "  return paths, corrected_samples\n",
    "\n",
    "\n",
    "train_img_paths, train_labels = get_image_paths_and_labels(train_samples)\n",
    "validation_img_paths, validation_labels = get_image_paths_and_labels(validation_samples)\n",
    "test_img_paths, test_labels = get_image_paths_and_labels(test_samples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gA9ZdKXmMnnS",
    "outputId": "7aa8310f-bbdf-41b2-f272-80dd63651266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./words\\\\e04\\\\e04-030\\\\e04-030-04-08.png', './words\\\\k02\\\\k02-102\\\\k02-102-05-03.png', './words\\\\a01\\\\a01-082u\\\\a01-082u-01-04.png', './words\\\\m01\\\\m01-000\\\\m01-000-07-00.png', './words\\\\g01\\\\g01-031\\\\g01-031-07-06.png', './words\\\\f07\\\\f07-081b\\\\f07-081b-01-06.png', './words\\\\n03\\\\n03-082\\\\n03-082-04-03.png', './words\\\\g06\\\\g06-018c\\\\g06-018c-04-05.png', './words\\\\g06\\\\g06-011j\\\\g06-011j-06-06.png', './words\\\\f04\\\\f04-024\\\\f04-024-01-06.png']\n",
      "['e04-030-04-08 ok 170 1489 1499 120 39 JJ sure', 'k02-102-05-03 ok 182 836 1623 69 52 PP3A he', 'a01-082u-01-04 ok 172 1582 1043 234 88 IN during', 'm01-000-07-00 ok 196 339 1998 75 107 INO of', 'g01-031-07-06 ok 152 1912 2038 167 59 NN booty', 'f07-081b-01-06 ok 168 1366 924 350 88 NN gastronomy', 'n03-082-04-03 ok 165 992 1414 118 135 NN boy', 'g06-018c-04-05 ok 182 1298 1438 96 58 ATI The', 'g06-011j-06-06 ok 182 1222 1785 146 95 CC and', 'f04-024-01-06 ok 183 1104 981 60 70 IN in']\n"
     ]
    }
   ],
   "source": [
    "print(train_img_paths[0:10])\n",
    "print(train_labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jib0CaHT-6u"
   },
   "source": [
    "# **Feature Extraction**\n",
    "Now we will take the actual label word from sample_lables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8D4QnrZ-QGh",
    "outputId": "8e810dec-2103-4e5f-bca3-742746805a94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Length: 21\n",
      "Vocab size: 78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sure',\n",
       " 'he',\n",
       " 'during',\n",
       " 'of',\n",
       " 'booty',\n",
       " 'gastronomy',\n",
       " 'boy',\n",
       " 'The',\n",
       " 'and',\n",
       " 'in']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the maximum length and the size of the vocabulary in the training data.\n",
    "train_labels_cleaned = []\n",
    "characters = set()\n",
    "max_len = 0\n",
    "\n",
    "for label in train_labels:\n",
    "  label = label.split(\" \")[-1].strip()\n",
    "  for char in label:\n",
    "    characters.add(char)\n",
    "\n",
    "  max_len = max(max_len, len(label))\n",
    "  train_labels_cleaned.append(label)\n",
    "\n",
    "print(\"Maximum Length:\",max_len)\n",
    "print(\"Vocab size:\",len(characters))\n",
    "\n",
    "#checking\n",
    "\n",
    "train_labels_cleaned[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qScbY15pX5rc"
   },
   "source": [
    "**Now cleaning the validation and the test labels as well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eYn1Bgd8W0ig"
   },
   "outputs": [],
   "source": [
    "def clean_labels(labels):\n",
    "  cleaned_labels = []\n",
    "  for label in labels:\n",
    "    label = label.split(\" \")[-1].strip()\n",
    "    cleaned_labels.append(label)\n",
    "\n",
    "  return cleaned_labels\n",
    "\n",
    "validation_labels_cleaned= clean_labels(validation_labels)\n",
    "test_labels_cleaned = clean_labels(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Co8ih0u2syY"
   },
   "source": [
    "# **Designing a Character vocabulary.**\n",
    "\n",
    "The label of the data are character which cannot be directly fit to data instead we need to convert this character to integer. So we will be converting the character label to integer using **StringLookup** by tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLugUbu_30KX",
    "outputId": "597c5316-b017-46c7-ba86-da599f46b311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q', 'A', 'E', 'C', 'B', ';', 'K', '&', '3', 'P', 'x', '1', 'X', 'F', 'r', '4', 'z', 'V', 'w', 'M', 'R', 'v', '7', 'i', 'o', '.', '9', 'k', 'y', 't', 'L', 'q', '\"', '+', 'd', 'b', 'a', '/', 'e', '?', '!', '8', 'n', 'N', 'c', 'j', '0', '(', 'm', 'W', 'Y', '2', ')', 'T', 'O', '6', '-', ',', 'D', 'H', '#', 'p', 's', 'G', 'I', 'J', ':', '*', \"'\", 'f', 'Z', 'l', 'S', 'U', 'h', 'u', '5', 'g'}\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "print(characters)\n",
    "print(len(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Iata-zfwYt2R"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "#TensorFlow will automatically decide the optimal values for characters.\n",
    "\n",
    "#mapping character to integer\n",
    "char_to_num = StringLookup(vocabulary = list(characters), mask_token=None)\n",
    "\n",
    "#mapping integers back to original characters.\n",
    "num_to_char = StringLookup(vocabulary = char_to_num.get_vocabulary(), mask_token=None, invert = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " 'Q',\n",
       " 'A',\n",
       " 'E',\n",
       " 'C',\n",
       " 'B',\n",
       " ';',\n",
       " 'K',\n",
       " '&',\n",
       " '3',\n",
       " 'P',\n",
       " 'x',\n",
       " '1',\n",
       " 'X',\n",
       " 'F',\n",
       " 'r',\n",
       " '4',\n",
       " 'z',\n",
       " 'V',\n",
       " 'w',\n",
       " 'M',\n",
       " 'R',\n",
       " 'v',\n",
       " '7',\n",
       " 'i',\n",
       " 'o',\n",
       " '.',\n",
       " '9',\n",
       " 'k',\n",
       " 'y',\n",
       " 't',\n",
       " 'L',\n",
       " 'q',\n",
       " '\"',\n",
       " '+',\n",
       " 'd',\n",
       " 'b',\n",
       " 'a',\n",
       " '/',\n",
       " 'e',\n",
       " '?',\n",
       " '!',\n",
       " '8',\n",
       " 'n',\n",
       " 'N',\n",
       " 'c',\n",
       " 'j',\n",
       " '0',\n",
       " '(',\n",
       " 'm',\n",
       " 'W',\n",
       " 'Y',\n",
       " '2',\n",
       " ')',\n",
       " 'T',\n",
       " 'O',\n",
       " '6',\n",
       " '-',\n",
       " ',',\n",
       " 'D',\n",
       " 'H',\n",
       " '#',\n",
       " 'p',\n",
       " 's',\n",
       " 'G',\n",
       " 'I',\n",
       " 'J',\n",
       " ':',\n",
       " '*',\n",
       " \"'\",\n",
       " 'f',\n",
       " 'Z',\n",
       " 'l',\n",
       " 'S',\n",
       " 'U',\n",
       " 'h',\n",
       " 'u',\n",
       " '5',\n",
       " 'g']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_num.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pbDsOqX42IX",
    "outputId": "cdbdc8cd-1f85-4bc8-bc8e-daf4a465dde4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(char_to_num(tf.constant([\"A\"])))\n",
    "#here A got value 76\n",
    "# the values nott in the characters  list gets value 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "168OFlOz6py5"
   },
   "source": [
    "# **Image Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jy2d-3K6wFH"
   },
   "source": [
    "### **Resizing image using Padding**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-XczJkI8SRd"
   },
   "source": [
    "Instead of using square images, many OCR models use rectangular images. You'll see why when we look at some examples from the dataset.Resizing square images without keeping their proportions doesn't distort them much, but this isn't true for rectangular images. However, we need to resize images to the same size for mini-batching. So, we must resize them while:\n",
    "\n",
    "1. Keeping their aspect ratio the same.\n",
    "2. Ensuring the image content remains unaffected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "B3yuuCB98NkS"
   },
   "outputs": [],
   "source": [
    "def distortion_free_resize(image, img_size):\n",
    "  w,h = img_size\n",
    "  image = tf.image.resize(image, size=(h,w),preserve_aspect_ratio=True)  # size paraeter takes height first and then width\n",
    "  #The resulting image might not be exactly (h,w) pixels but will fit within these dimensions without any distortion.\n",
    "  # if any pixel left we will use padding\n",
    "\n",
    "  #checking the padding height and width\n",
    "  pad_height = h-tf.shape(image)[0]\n",
    "  pad_width = w-tf.shape(image)[1]\n",
    "\n",
    "  #Only necessary if you want to do some amount of padding on both sides.\n",
    "  if pad_height % 2 !=0:\n",
    "    height=pad_height//2\n",
    "    pad_height_top = height +1\n",
    "    pad_height_bottom = height\n",
    "  else:\n",
    "    pad_height_top = pad_height_bottom = pad_height // 2\n",
    "\n",
    "  if pad_width %2 != 0:\n",
    "    width = pad_width //2\n",
    "    pad_width_left = width +1\n",
    "    pad_width_right = width\n",
    "  else:\n",
    "    pad_width_left = pad_width_right = pad_width //2\n",
    "\n",
    "  image = tf.pad(\n",
    "      image,\n",
    "      paddings = [\n",
    "          [pad_height_top, pad_height_bottom],\n",
    "          [pad_width_left, pad_width_right],\n",
    "          [0,0],\n",
    "      ],\n",
    "\n",
    "  )\n",
    "  image = tf.transpose(image, perm=[1,0,2])\n",
    "  image = tf.image.flip_left_right(image)\n",
    "  # because tf.resize uses (h,w) way\n",
    "  return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gy-ryctyK7gf"
   },
   "source": [
    "# **Putting all together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VhxZTmzjKq2U"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "padding_token = 99\n",
    "image_width = 128\n",
    "image_height = 32\n",
    "\n",
    "def preprocessing_image(image_path, img_size=(image_width, image_height)):\n",
    "  image = tf.io.read_file(image_path)\n",
    "  image = tf.image.decode_png(image,channels=1) # decode the png_encoded images into tensor , channel 1 for gray scale\n",
    "  image = distortion_free_resize(image,img_size)\n",
    "  image = tf.cast(image, tf.float32)/255.0  # data type conversion in tensor\n",
    "  return image\n",
    "\n",
    "def vectorize_label(label):\n",
    "  label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "  length = tf.shape(label)[0]\n",
    "  pad_amount = max_len-length\n",
    "  label = tf.pad(label, paddings=[[0,pad_amount]], constant_values = padding_token)\n",
    "  return label\n",
    "\n",
    "def decode_label(encoded_label):\n",
    "    unpadded_label = tf.boolean_mask(encoded_label, encoded_label != padding_token)\n",
    "    decoded_word = num_to_char(unpadded_label)\n",
    "    return tf.strings.reduce_join(decoded_word).numpy().decode(\"utf-8\")\n",
    "\n",
    "def process_images_labels(image_path, label):\n",
    "  image = preprocessing_image(image_path)\n",
    "  label = vectorize_label(label)\n",
    "  return {\"image\":image, \"label\":label}\n",
    "\n",
    "def prepare_dataset(image_paths, labels):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels)).map(process_images_labels, num_parallel_calls=AUTOTUNE)\n",
    "  return dataset.batch(batch_size).cache().prefetch(AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39I3P4PBrFeB"
   },
   "source": [
    "**Preparing tf.data.Dataset objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "y7BIP61-rDHK"
   },
   "outputs": [],
   "source": [
    "train_ds = prepare_dataset(train_img_paths, train_labels_cleaned)\n",
    "validation_ds = prepare_dataset(validation_img_paths, validation_labels_cleaned)\n",
    "test_ds = prepare_dataset(test_img_paths, test_labels_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9D18E4wv8HHf"
   },
   "source": [
    "### **Visualizing a few Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OVYlAafKFsOh",
    "outputId": "813e0ce0-acb5-494b-c380-cf4f38b50e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec={'image': TensorSpec(shape=(None, None, None, 1), dtype=tf.float32, name=None), 'label': TensorSpec(shape=(None, None), dtype=tf.int64, name=None)}>\n"
     ]
    }
   ],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ccSST3xswEh",
    "outputId": "7339b75e-f920-48cb-bd09-5ea115a9fcb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor details:\n",
      "Shape: (32, 128, 32, 1)\n",
      "Dtype: <dtype: 'float32'>\n",
      "Example data: [[0.7879308 ]\n",
      " [0.41669166]\n",
      " [0.33433872]\n",
      " [0.5890069 ]\n",
      " [0.9208927 ]\n",
      " [0.97726715]\n",
      " [0.9816583 ]\n",
      " [0.9833934 ]\n",
      " [0.9833934 ]\n",
      " [0.9810781 ]\n",
      " [0.97783613]\n",
      " [0.97730905]\n",
      " [0.9812506 ]\n",
      " [0.9661871 ]\n",
      " [0.9751751 ]\n",
      " [0.9833934 ]\n",
      " [0.9839542 ]\n",
      " [0.9843137 ]\n",
      " [0.9832958 ]\n",
      " [0.9818653 ]\n",
      " [0.98383915]\n",
      " [0.9843137 ]\n",
      " [0.98346525]\n",
      " [0.98346967]\n",
      " [0.97938985]\n",
      " [0.9790454 ]\n",
      " [0.98290753]\n",
      " [0.9843137 ]\n",
      " [0.9838823 ]\n",
      " [0.98247296]\n",
      " [0.9837097 ]\n",
      " [0.9843137 ]]\n",
      "Label tensor details:\n",
      "Shape: (32, 21)\n",
      "Dtype: <dtype: 'int64'>\n",
      "Example data: [63 76 15 39 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99]\n"
     ]
    }
   ],
   "source": [
    "train_ds_iter = iter(train_ds)\n",
    "batch = next(train_ds_iter)\n",
    "image_tensor = batch['image']\n",
    "print(\"Image tensor details:\")\n",
    "print(f\"Shape: {image_tensor.shape}\")\n",
    "print(f\"Dtype: {image_tensor.dtype}\")\n",
    "print(f\"Example data: {image_tensor.numpy()[0][20]}\")  # Print the first image in the batch\n",
    "\n",
    "label_tensor = batch['label']\n",
    "print(\"Label tensor details:\")\n",
    "print(f\"Shape: {label_tensor.shape}\")\n",
    "print(f\"Dtype: {label_tensor.dtype}\")\n",
    "print(f\"Example data: {label_tensor.numpy()[0]}\")  # Print the first label in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-GlEFokbh_K"
   },
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc5e86WvbnNb"
   },
   "source": [
    "We will be using **CTC(Connectionist Temporal Classification)**  loss as an endpoint layer.CTC is a neural network output layer designed to solve sequence-to-sequence problems where the alignment between input and output sequences is unknown.\n",
    "\n",
    "**CNNs** (for spatial feature extraction) and **RNNs **(for sequential modeling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncm5GmpRbmvK",
    "outputId": "c9726f23-04d4-43ce-d08d-dd70d9fef476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"handwriting_recognizer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image (InputLayer)             [(None, 128, 32, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 128, 32, 32)  320         ['image[0][0]']                  \n",
      "                                                                                                  \n",
      " pool1 (MaxPooling2D)           (None, 64, 16, 32)   0           ['Conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " Conv2 (Conv2D)                 (None, 64, 16, 64)   18496       ['pool1[0][0]']                  \n",
      "                                                                                                  \n",
      " pool2 (MaxPooling2D)           (None, 32, 8, 64)    0           ['Conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 32, 512)      0           ['pool2[0][0]']                  \n",
      "                                                                                                  \n",
      " dense1 (Dense)                 (None, 32, 64)       32832       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 32, 64)       0           ['dense1[0][0]']                 \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirectional  (None, 32, 256)     197632      ['dropout_2[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirectional  (None, 32, 128)     164352      ['bidirectional_4[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " label (InputLayer)             [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " dense2 (Dense)                 (None, 32, 81)       10449       ['bidirectional_5[0][0]']        \n",
      "                                                                                                  \n",
      " ctc_loss (CTCLayer)            (None, 32, 81)       0           ['label[0][0]',                  \n",
      "                                                                  'dense2[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 424,081\n",
      "Trainable params: 424,081\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class CTCLayer(keras.layers.Layer):\n",
    "  def __init__(self, name=None):\n",
    "    super().__init__(name=name)\n",
    "    self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "  def call(self,y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[0], dtype = \"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len,1), dtype=\"int64\")\n",
    "    loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "    self.add_loss(loss)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def build_model():\n",
    "  # Inputs to the model. These are used when the input and output have different structures.\n",
    "  input_img = keras.Input(shape=(image_width, image_height,1), name = \"image\")\n",
    "  labels = keras.layers.Input(name=\"label\", shape=(None,))\n",
    "\n",
    "  # first conv block.\n",
    "  x = keras.layers.Conv2D(32, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\", name=\"Conv1\")(input_img)\n",
    "  x = keras.layers.MaxPooling2D((2,2), name=\"pool1\")(x)\n",
    "\n",
    "  #Second conv block.\n",
    "  x = keras.layers.Conv2D(64, (3,3), activation = \"relu\", kernel_initializer=\"he_normal\", padding=\"same\",name=\"Conv2\")(x)\n",
    "  x = keras.layers.MaxPooling2D((2,2), name = \"pool2\")(x)\n",
    "\n",
    "  # Here we have used two max pool with pool size and strides 2.\n",
    "  # Hence, downsampled feature maps are 4x smaller.\n",
    "  # The number of filter in the last layer is 64 .\n",
    "  # Reshape accordingly before passing the output to the RNN part of the model.\n",
    "\n",
    "  #If the CNN output is of shape (batch_size, new_width, new_height, num_channels), the reshaping converts it to (batch_size, new_width, new_height * num_channels).\n",
    "  #This is necessary to convert the output of the CNN into a format suitable for the subsequent Dense layer and RNN.\n",
    "  #primary purpose of Dropout is to prevent overfitting and generalize during the training of the neural network.\n",
    "  new_shape = ((image_width // 4), (image_height // 4)* 64)\n",
    "  x = keras.layers.Reshape(target_shape = new_shape, name=\"reshape\")(x)\n",
    "  x = keras.layers.Dense(64, activation= \"relu\", name=\"dense1\")(x)\n",
    "  x = keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "  #RNNs\n",
    "  x = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True, dropout = 0.25))(x)\n",
    "  x = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences = True, dropout = 0.25))(x)\n",
    "\n",
    "  # +2 is to account for the two special tokens introduced by the CTC loss.\n",
    "  # The recommendation is from https://git.io/J0eXP.\n",
    "  x = keras.layers.Dense(len(char_to_num.get_vocabulary()) + 2, activation = \"softmax\", name=\"dense2\")(x)\n",
    "\n",
    "  #Add CTC layer for calculating CTC loss at each step.\n",
    "  output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
    "\n",
    "  #Define the model.\n",
    "  model = keras.models.Model(\n",
    "      inputs = [input_img, labels], outputs = output, name=\"handwriting_recognizer\")\n",
    "  #optimizer\n",
    "  opt = keras.optimizers.Adam()\n",
    "  #compile the model and return\n",
    "  model.compile(optimizer = opt)\n",
    "  return model\n",
    "\n",
    "\n",
    "#Get model\n",
    "model = build_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIUCob8cHwgQ"
   },
   "source": [
    "# **Edit Distance as Evaluation Metrix**\n",
    "Edit distance is the most widely used matric for evaluating OCR models. It is also known as Levenshtein distance, is a metric for evaluating the similarity between two sequences by counting the minimum number of operations required to transform one sequence into the other.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "KFHZY5OzM3D_"
   },
   "outputs": [],
   "source": [
    "# we first segregate the validation images and their labels for convenience.\n",
    "with tf.device('/GPU:0'):\n",
    "  validation_images = []\n",
    "  validation_labels = []\n",
    "\n",
    "  for batch in validation_ds:\n",
    "    validation_images.append(batch[\"image\"])\n",
    "    validation_labels.append(batch[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "vCxyLq5_LBG_"
   },
   "outputs": [],
   "source": [
    "# Now, we create a callback to monitor the edit distances.\n",
    "\n",
    "\n",
    "def calculate_edit_distance(labels, predictions):\n",
    "  #converting the labels into a sparse tensors.\n",
    "  saprse_labels = tf.cast(tf.sparse.from_dense(labels), dtype=tf.int64)\n",
    "\n",
    "  input_len = np.ones(predictions.shape[0])*predictions.shape[1]\n",
    "  predictions_decoded = keras.backend.ctc_decode(predictions, input_length=input_len, greedy = True)[0][0][:,:max_len]\n",
    "  sparse_predictions = tf.cast(tf.sparse.from_dense(predictions_decoded),dtype=tf.int64)\n",
    "\n",
    "  # compute individual edit distance and average them out.\n",
    "  edit_distances = tf.edit_distance(\n",
    "      sparse_predictions, saprse_labels, normalize=False\n",
    "  )\n",
    "  return tf.reduce_mean(edit_distances)\n",
    "\n",
    "\n",
    "class EditDistanceCallback(keras.callbacks.Callback):\n",
    "  def __init__(self, pred_model):\n",
    "    super().__init__()\n",
    "    self.prediction_model = pred_model\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    edit_distance = []\n",
    "\n",
    "    for i in range(len(validation_images)):\n",
    "      lables = validation_labels[i]\n",
    "      predictions = self.prediction_model.predict(validation_images[i])\n",
    "      edit_distance.append(calculate_edit_distance(labels, predictions).numpy())\n",
    "    print(\n",
    "        f\"Mean edit distance for epoch {epoch+1}: {np.mean(edit_distance):.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Character Error Rate as Evaluation Metrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_batch_prediction(pred):\n",
    "  input_len = np.ones(pred.shape[0])*pred.shape[1]\n",
    "\n",
    "  # Use greedy search For complex tasks , you can use beam search.\n",
    "  results = keras.backend.ctc_decode(pred, input_length=input_len, greedy = True )[0][0][:,:max_len]\n",
    "  #Iterate over the results and get back the text.\n",
    "  output_text = []\n",
    "  for res in results:\n",
    "    res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n",
    "    res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "    output_text.append(res)\n",
    "  return output_text\n",
    "\n",
    "def calculate_character_error_rate(gt, pred):\n",
    "    total_errors = 0\n",
    "    total_characters = 0\n",
    "    for word_gt, word_pred in zip(gt, pred):\n",
    "        # Compute the edit distance for each word\n",
    "        distance = editdistance.eval(word_gt, word_pred)\n",
    "        total_errors += distance\n",
    "        total_characters += len(word_gt)\n",
    "    # Calculate CER\n",
    "    return total_errors / total_characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterErrorRateCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, prediction_model):\n",
    "        super().__init__()\n",
    "        self.prediction_model = prediction_model\n",
    "        self.cer_per_epoch = []  \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        label_words = []\n",
    "        predicted_words = []\n",
    "        for batch in test_ds.take(2):\n",
    "            batch_images = batch[\"image\"]\n",
    "            batch_labels = batch[\"label\"]\n",
    "\n",
    "            for word in batch_labels:\n",
    "                label_words.append(decode_label(word))\n",
    "            preds = self.prediction_model.predict(batch_images)\n",
    "            predi_word = decode_batch_prediction(preds)\n",
    "            predicted_words.append(predi_word)\n",
    "\n",
    "        predict_words = [item for sublist in predicted_words for item in sublist]\n",
    "        CERs = calculate_character_error_rate(label_words, predict_words)\n",
    "        self.cer_per_epoch.append(f\"{CERs:.4f}\")\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}: Mean CER = {np.mean(CERs):.4f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QhFfn9GK__C"
   },
   "source": [
    "# **Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yT4Kmhum2pP4",
    "outputId": "71ce8e08-393b-41d3-9123-1cf05bc23a8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 128, 32, 1), dtype=tf.float32, name='image'), name='image', description=\"created by layer 'image'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 32, 81), dtype=tf.float32, name=None), name='dense2/Softmax:0', description=\"created by layer 'dense2'\")\n",
      "<keras.engine.functional.Functional object at 0x000001D3AF0BF5E0>\n",
      "Epoch 1/4\n",
      "1/1 [==============================] - 3s 3s/step 0s - loss: 12.88\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "Epoch 1: Mean CER = 0.7462\n",
      "2713/2713 [==============================] - 954s 349ms/step - loss: 12.8888 - val_loss: 11.1299\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 86ms/steps - loss: 9.75\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "Epoch 2: Mean CER = 0.5923\n",
      "2713/2713 [==============================] - 702s 259ms/step - loss: 9.7527 - val_loss: 8.2947\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 93ms/steps - loss: 7.50\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "Epoch 3: Mean CER = 0.4154\n",
      "2713/2713 [==============================] - 687s 253ms/step - loss: 7.5086 - val_loss: 5.8124\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 89ms/steps - loss: 5.59\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "Epoch 4: Mean CER = 0.2885\n",
      "2713/2713 [==============================] - 698s 257ms/step - loss: 5.5974 - val_loss: 4.3263\n"
     ]
    }
   ],
   "source": [
    "epochs = 70\n",
    "\n",
    "model = build_model()\n",
    "input_layer = model.inputs[0]\n",
    "print(input_layer)\n",
    "output_layer = model.get_layer(name=\"dense2\").output\n",
    "print(output_layer)\n",
    "prediction_model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "print(prediction_model)\n",
    "\n",
    "CER_callback = CharacterErrorRateCallback(prediction_model)\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data = validation_ds,\n",
    "    epochs = epochs,\n",
    "    callbacks=[CER_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CER and WER plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d39fdcb160>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Plot Training and Validation Loss\n",
    "epochs_range = range(1, epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Training and Validation Loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, history.history['loss'], label='Training Loss')\n",
    "plt.plot(epochs_range, history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Character Error Rate (CER)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, CER_callback.cer_per_epoch, label='Character Error Rate (CER)', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('CER')\n",
    "plt.title('Character Error Rate per Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model.save('model_V80.keras')\n",
    "prediction_model.save('model_V80.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_words = []\n",
    "predicted_words = []\n",
    "for batch in test_ds:\n",
    "    batch_images = batch[\"image\"]\n",
    "    batch_labels = batch[\"label\"]\n",
    "\n",
    "    for word in batch_labels:\n",
    "        label_words.append(decode_label(word))\n",
    "    preds = prediction_model.predict(batch_images)\n",
    "    predi_word = decode_batch_prediction(preds)\n",
    "    predicted_words.append(predi_word)\n",
    "\n",
    "predict_words = [item for sublist in predicted_words for item in sublist]\n",
    "CERs = calculate_character_error_rate(label_words, predict_words)\n",
    "print(\"The CER for train dataset be : \", CERs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "supreme",
   "language": "python",
   "name": "supreme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
